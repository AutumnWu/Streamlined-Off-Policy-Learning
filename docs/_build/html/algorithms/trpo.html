

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Trust Region Policy Optimization &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Proximal Policy Optimization" href="ppo.html" />
    <link rel="prev" title="Vanilla Policy Gradient" href="vpg.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">Part 2: Kinds of RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">Key Papers in Deep RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trust Region Policy Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-equations">Key Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exploration-vs-exploitation">Exploration vs. Exploitation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#documentation">Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#saved-model-contents">Saved Model Contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#relevant-papers">Relevant Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-these-papers">Why These Papers?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-public-implementations">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Trust Region Policy Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/trpo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="trust-region-policy-optimization">
<h1><a class="toc-backref" href="#id4">Trust Region Policy Optimization</a><a class="headerlink" href="#trust-region-policy-optimization" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#trust-region-policy-optimization" id="id4">Trust Region Policy Optimization</a><ul>
<li><a class="reference internal" href="#background" id="id5">Background</a><ul>
<li><a class="reference internal" href="#quick-facts" id="id6">Quick Facts</a></li>
<li><a class="reference internal" href="#key-equations" id="id7">Key Equations</a></li>
<li><a class="reference internal" href="#exploration-vs-exploitation" id="id8">Exploration vs. Exploitation</a></li>
<li><a class="reference internal" href="#pseudocode" id="id9">Pseudocode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#documentation" id="id10">Documentation</a><ul>
<li><a class="reference internal" href="#saved-model-contents" id="id11">Saved Model Contents</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references" id="id12">References</a><ul>
<li><a class="reference internal" href="#relevant-papers" id="id13">Relevant Papers</a></li>
<li><a class="reference internal" href="#why-these-papers" id="id14">Why These Papers?</a></li>
<li><a class="reference internal" href="#other-public-implementations" id="id15">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<h2><a class="toc-backref" href="#id5">Background</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>(Previously: <a class="reference external" href="../algorithms/vpg.html#background">Background for VPG</a>)</p>
<p>TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms of <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL-Divergence</a>, a measure of (something like, but not exactly) distance between probability distributions.</p>
<p>This is different from normal policy gradient, which keeps new and old policies close in parameter space. But even seemingly small differences in parameter space can have very large differences in performance&#8212;so a single bad step can collapse the policy performance. This makes it dangerous to use large step sizes with vanilla policy gradients, thus hurting its sample efficiency. TRPO nicely avoids this kind of collapse, and tends to quickly and monotonically improve performance.</p>
<div class="section" id="quick-facts">
<h3><a class="toc-backref" href="#id6">Quick Facts</a><a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>TRPO is an on-policy algorithm.</li>
<li>TRPO can be used for environments with either discrete or continuous action spaces.</li>
<li>The Spinning Up implementation of TRPO supports parallelization with MPI.</li>
</ul>
</div>
<div class="section" id="key-equations">
<h3><a class="toc-backref" href="#id7">Key Equations</a><a class="headerlink" href="#key-equations" title="Permalink to this headline">¶</a></h3>
<p>Let <img class="math" src="../_images/math/80088cfe6126980142c5447a9cb12f69ee7fa333.svg" alt="\pi_{\theta}"/> denote a policy with parameters <img class="math" src="../_images/math/1a20bd03ccceae216a40cb69d3fb7a3970f6f275.svg" alt="\theta"/>. The theoretical TRPO update is:</p>
<div class="math">
<p><img src="../_images/math/49fb48638180b9db4e6fa3008314fe6a1f2e04fe.svg" alt="\theta_{k+1} = \arg \max_{\theta} \; &amp; {\mathcal L}(\theta_k, \theta) \\
\text{s.t.} \; &amp; \bar{D}_{KL}(\theta || \theta_k) \leq \delta"/></p>
</div><p>where <img class="math" src="../_images/math/12d9ee2e680e6e345d3cede2c0d7376073bae206.svg" alt="{\mathcal L}(\theta_k, \theta)"/> is the <em>surrogate advantage</em>, a measure of how policy <img class="math" src="../_images/math/80088cfe6126980142c5447a9cb12f69ee7fa333.svg" alt="\pi_{\theta}"/> performs relative to the old policy <img class="math" src="../_images/math/c0489440be42a601521652989352e32af063723c.svg" alt="\pi_{\theta_k}"/> using data from the old policy:</p>
<div class="math">
<p><img src="../_images/math/7d5488f060af7629fc8ced8b21a37e5739f741ad.svg" alt="{\mathcal L}(\theta_k, \theta) = \underE{s,a \sim \pi_{\theta_k}}{
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a)
    },"/></p>
</div><p>and <img class="math" src="../_images/math/8b59be8adda60a88a54f45e1499aede82067971f.svg" alt="\bar{D}_{KL}(\theta || \theta_k)"/> is an average KL-divergence between policies across states visited by the old policy:</p>
<div class="math">
<p><img src="../_images/math/8bb33077ffd11e60d3c6eb0a588e42b69acde4d3.svg" alt="\bar{D}_{KL}(\theta || \theta_k) = \underE{s \sim \pi_{\theta_k}}{
    D_{KL}\left(\pi_{\theta}(\cdot|s) || \pi_{\theta_k} (\cdot|s) \right)
}."/></p>
</div><div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">The objective and constraint are both zero when <img class="math" src="../_images/math/256fb885ac1ae20b17a0c7b4ff2137d3fde2abf8.svg" alt="\theta = \theta_k"/>. Furthermore, the gradient of the constraint with respect to <img class="math" src="../_images/math/1a20bd03ccceae216a40cb69d3fb7a3970f6f275.svg" alt="\theta"/> is zero when <img class="math" src="../_images/math/256fb885ac1ae20b17a0c7b4ff2137d3fde2abf8.svg" alt="\theta = \theta_k"/>. Proving these facts requires some subtle command of the relevant math&#8212;it&#8217;s an exercise worth doing, whenever you feel ready!</p>
</div>
<p>The theoretical TRPO update isn&#8217;t the easiest to work with, so TRPO makes some approximations to get an answer quickly. We Taylor expand the objective and constraint to leading order around <img class="math" src="../_images/math/f641937e50bf17c2fcf11e6c8fb60a3cc36d2793.svg" alt="\theta_k"/>:</p>
<div class="math">
<p><img src="../_images/math/a12afc0aa92f0de8203fd714993eb3ec88dfe270.svg" alt="{\mathcal L}(\theta_k, \theta) &amp;\approx g^T (\theta - \theta_k) \\
\bar{D}_{KL}(\theta || \theta_k) &amp; \approx \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k)"/></p>
</div><p>resulting in an approximate optimization problem,</p>
<div class="math">
<p><img src="../_images/math/37a2fc91d1e23c341896b067aa9a7c85b4cb0908.svg" alt="\theta_{k+1} = \arg \max_{\theta} \; &amp; g^T (\theta - \theta_k) \\
\text{s.t.} \; &amp; \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k) \leq \delta."/></p>
</div><div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">By happy coincidence, the gradient <img class="math" src="../_images/math/c06262884339ba7a3058d27d76c58f8f0c8dc4bb.svg" alt="g"/> of the surrogate advantage function with respect to <img class="math" src="../_images/math/1a20bd03ccceae216a40cb69d3fb7a3970f6f275.svg" alt="\theta"/>, evaluated at <img class="math" src="../_images/math/256fb885ac1ae20b17a0c7b4ff2137d3fde2abf8.svg" alt="\theta = \theta_k"/>, is exactly equal to the policy gradient, <img class="math" src="../_images/math/f839e53d1ccacb63d4d86a8be348d9adc86d723a.svg" alt="\nabla_{\theta} J(\pi_{\theta})"/>! Try proving this, if you feel comfortable diving into the math.</p>
</div>
<p>This approximate problem can be analytically solved by the methods of Lagrangian duality <a class="footnote-reference" href="#id2" id="id1">[1]</a>, yielding the solution:</p>
<div class="math">
<p><img src="../_images/math/46d48cc9eeb79d52d2f09900fd191f65108f7326.svg" alt="\theta_{k+1} = \theta_k + \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g."/></p>
</div><p>If we were to stop here, and just use this final result, the algorithm would be exactly calculating the <a class="reference external" href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">Natural Policy Gradient</a>. A problem is that, due to the approximation errors introduced by the Taylor expansion, this may not satisfy the KL constraint, or actually improve the surrogate advantage. TRPO adds a modification to this update rule: a backtracking line search,</p>
<div class="math">
<p><img src="../_images/math/e14fae1f7f9dd024694a3a7598a772491a654c17.svg" alt="\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g,"/></p>
</div><p>where <img class="math" src="../_images/math/da24b56ee04420fc293f65deadefc862f651ac4c.svg" alt="\alpha \in (0,1)"/> is the backtracking coefficient, and <img class="math" src="../_images/math/78c461b0c8998c6936bbb50187bb91955b4785b6.svg" alt="j"/> is the smallest nonnegative integer such that <img class="math" src="../_images/math/70e714fda6cf3dbaa486eb712146e920d8c93e70.svg" alt="\pi_{\theta_{k+1}}"/> satisfies the KL constraint and produces a positive surrogate advantage.</p>
<p>Lastly: computing and storing the matrix inverse, <img class="math" src="../_images/math/c19686d29c8808f32acb34a25b652118f928195a.svg" alt="H^{-1}"/>, is painfully expensive when dealing with neural network policies with thousands or millions of parameters. TRPO sidesteps the issue by using the <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">conjugate gradient</a> algorithm to solve <img class="math" src="../_images/math/5b62d8ba7da612617b0d27c96721d9db4a80f20e.svg" alt="Hx = g"/> for <img class="math" src="../_images/math/2e031bbcf194184b9af9cba5a817d85007e83772.svg" alt="x = H^{-1} g"/>, requiring only a function which can compute the matrix-vector product <img class="math" src="../_images/math/c9673c4437ceb8dc9bedf413cae4773b4d9be489.svg" alt="Hx"/> instead of computing and storing the whole matrix <img class="math" src="../_images/math/b9d29b0250dbf71e8858eba83d7d2bf4f6c32e4a.svg" alt="H"/> directly. This is not too hard to do: we set up a symbolic operation to calculate</p>
<div class="math">
<p><img src="../_images/math/8d7cb2dcf48ef522fff82ce155d3a57748861642.svg" alt="Hx = \nabla_{\theta} \left( \left(\nabla_{\theta} \bar{D}_{KL}(\theta || \theta_k)\right)^T x \right),"/></p>
</div><p>which gives us the correct output without computing the whole matrix.</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>See <a class="reference external" href="http://stanford.edu/~boyd/cvxbook/">Convex Optimization</a> by Boyd and Vandenberghe, especially chapters 2 through 5.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="exploration-vs-exploitation">
<h3><a class="toc-backref" href="#id8">Exploration vs. Exploitation</a><a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>TRPO trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima.</p>
</div>
<div class="section" id="pseudocode">
<h3><a class="toc-backref" href="#id9">Pseudocode</a><a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><img src="../_images/math/fece98b123fdbe62167dade95a7c53d836ddc5a1.svg" alt="\begin{algorithm}[H]
    \caption{Trust Region Policy Optimization}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \STATE Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Use the conjugate gradient algorithm to compute
        \begin{equation*}
        \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k,
        \end{equation*}
        where $\hat{H}_k$ is the Hessian of the sample average KL-divergence.
    \STATE Update the policy by backtracking line search with
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha^j \sqrt{ \frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k,
        \end{equation*}
        where $j \in \{0, 1, 2, ... K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}"/></p>
</div></div>
</div>
<div class="section" id="documentation">
<h2><a class="toc-backref" href="#id10">Documentation</a><a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.trpo">
<code class="descclassname">spinup.</code><code class="descname">trpo</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=50</em>, <em>gamma=0.99</em>, <em>delta=0.01</em>, <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>, <em>damping_coeff=0.1</em>, <em>cg_iters=10</em>, <em>backtrack_iters=10</em>, <em>backtrack_coeff=0.8</em>, <em>lam=0.97</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=10</em>, <em>algo='trpo'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/trpo/trpo.html#trpo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.trpo" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="18%" />
<col width="24%" />
<col width="59%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">logp</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of taking actions <code class="docutils literal"><span class="pre">a_ph</span></code></div>
<div class="line">in states <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">info</span></code></td>
<td>N/A</td>
<td><div class="first last line-block">
<div class="line">A dict of any intermediate quantities</div>
<div class="line">(from calculating the policy or log</div>
<div class="line">probabilities) which are needed for</div>
<div class="line">analytically computing KL divergence.</div>
<div class="line">(eg sufficient statistics of the</div>
<div class="line">distributions)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">info_phs</span></code></td>
<td>N/A</td>
<td><div class="first last line-block">
<div class="line">A dict of placeholders for old values</div>
<div class="line">of the entries in <code class="docutils literal"><span class="pre">info</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">d_kl</span></code></td>
<td>()</td>
<td><div class="first last line-block">
<div class="line">A symbol for computing the mean KL</div>
<div class="line">divergence between the current policy</div>
<div class="line">(<code class="docutils literal"><span class="pre">pi</span></code>) and the old policy (as</div>
<div class="line">specified by the inputs to</div>
<div class="line"><code class="docutils literal"><span class="pre">info_phs</span></code>) over the batch of</div>
<div class="line">states given in <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>. (Critical: make sure</div>
<div class="line">to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to TRPO.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs of interaction (equivalent to
number of policy updates) to perform.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>delta</strong> (<em>float</em>) &#8211; KL-divergence limit for TRPO / NPG update.
(Should be small for stability. Values like 0.01, 0.05.)</li>
<li><strong>vf_lr</strong> (<em>float</em>) &#8211; Learning rate for value function optimizer.</li>
<li><strong>train_v_iters</strong> (<em>int</em>) &#8211; Number of gradient descent steps to take on
value function per epoch.</li>
<li><strong>damping_coeff</strong> (<em>float</em>) &#8211; <p>Artifact for numerical stability, should be
smallish. Adjusts Hessian-vector product calculation:</p>
<div class="math">
<p><img src="../_images/math/a74f4337a79aded69e079da83c07930fc60ed57b.svg" alt="Hv \rightarrow (\alpha I + H)v"/></p>
</div><p>where <img class="math" src="../_images/math/e37a3c8967bacd71f976963d8ff117f6dd0ad132.svg" alt="\alpha"/> is the damping coefficient.
Probably don&#8217;t play with this hyperparameter.</p>
</li>
<li><strong>cg_iters</strong> (<em>int</em>) &#8211; <p>Number of iterations of conjugate gradient to perform.
Increasing this will lead to a more accurate approximation
to <img class="math" src="../_images/math/f5d0f118aee5a364c9ef3d613ac60327f5287d5f.svg" alt="H^{-1} g"/>, and possibly slightly-improved performance,
but at the cost of slowing things down.</p>
<p>Also probably don&#8217;t play with this hyperparameter.</p>
</li>
<li><strong>backtrack_iters</strong> (<em>int</em>) &#8211; Maximum number of steps allowed in the
backtracking line search. Since the line search usually doesn&#8217;t
backtrack, and usually only steps back once when it does, this
hyperparameter doesn&#8217;t often matter.</li>
<li><strong>backtrack_coeff</strong> (<em>float</em>) &#8211; How far back to step during backtracking line
search. (Always between 0 and 1, usually above 0.5.)</li>
<li><strong>lam</strong> (<em>float</em>) &#8211; Lambda for GAE-Lambda. (Always between 0 and 1,
close to 1.)</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
<li><strong>algo</strong> &#8211; Either &#8216;trpo&#8217; or &#8216;npg&#8217;: this code supports both, since they are
almost the same.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="saved-model-contents">
<h3><a class="toc-backref" href="#id11">Saved Model Contents</a><a class="headerlink" href="#saved-model-contents" title="Permalink to this headline">¶</a></h3>
<p>The computation graph saved by the logger includes:</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>This saved model can be accessed either by</p>
<ul class="simple">
<li>running the trained policy with the <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> tool,</li>
<li>or loading the whole saved graph into a program with <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.</li>
</ul>
</div>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id12">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<div class="section" id="relevant-papers">
<h3><a class="toc-backref" href="#id13">Relevant Papers</a><a class="headerlink" href="#relevant-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, Schulman et al. 2015</li>
<li><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016</li>
<li><a class="reference external" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a>, Kakade and Langford 2002</li>
</ul>
</div>
<div class="section" id="why-these-papers">
<h3><a class="toc-backref" href="#id14">Why These Papers?</a><a class="headerlink" href="#why-these-papers" title="Permalink to this headline">¶</a></h3>
<p>Schulman 2015 is included because it is the original paper describing TRPO. Schulman 2016 is included because our implementation of TRPO makes use of Generalized Advantage Estimation for computing the policy gradient. Kakade and Langford 2002 is included because it contains theoretical results which motivate and deeply connect to the theoretical foundations of TRPO.</p>
</div>
<div class="section" id="other-public-implementations">
<h3><a class="toc-backref" href="#id15">Other Public Implementations</a><a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/trpo_mpi">Baselines</a></li>
<li><a class="reference external" href="https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py">ModularRL</a></li>
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/trpo.py">rllab</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ppo.html" class="btn btn-neutral float-right" title="Proximal Policy Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vpg.html" class="btn btn-neutral" title="Vanilla Policy Gradient" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>