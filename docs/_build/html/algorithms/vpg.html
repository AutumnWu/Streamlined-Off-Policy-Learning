

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vanilla Policy Gradient &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Trust Region Policy Optimization" href="trpo.html" />
    <link rel="prev" title="Benchmarks for Spinning Up Implementations" href="../spinningup/bench.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">User Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">Running Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">Experiment Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">Plotting Results</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction to RL</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">Part 1: Key Concepts in RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">Part 2: Kinds of RL Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">Part 3: Intro to Policy Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">Key Papers in Deep RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">Exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms Docs</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vanilla Policy Gradient</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-equations">Key Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exploration-vs-exploitation">Exploration vs. Exploitation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#documentation">Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#saved-model-contents">Saved Model Contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#relevant-papers">Relevant Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-these-papers">Why These Papers?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-public-implementations">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">Utilities Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">Plotter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">Run Utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Etc.</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">About the Author</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Vanilla Policy Gradient</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/vpg.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vanilla-policy-gradient">
<h1><a class="toc-backref" href="#id1">Vanilla Policy Gradient</a><a class="headerlink" href="#vanilla-policy-gradient" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#vanilla-policy-gradient" id="id1">Vanilla Policy Gradient</a><ul>
<li><a class="reference internal" href="#background" id="id2">Background</a><ul>
<li><a class="reference internal" href="#quick-facts" id="id3">Quick Facts</a></li>
<li><a class="reference internal" href="#key-equations" id="id4">Key Equations</a></li>
<li><a class="reference internal" href="#exploration-vs-exploitation" id="id5">Exploration vs. Exploitation</a></li>
<li><a class="reference internal" href="#pseudocode" id="id6">Pseudocode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#documentation" id="id7">Documentation</a><ul>
<li><a class="reference internal" href="#saved-model-contents" id="id8">Saved Model Contents</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references" id="id9">References</a><ul>
<li><a class="reference internal" href="#relevant-papers" id="id10">Relevant Papers</a></li>
<li><a class="reference internal" href="#why-these-papers" id="id11">Why These Papers?</a></li>
<li><a class="reference internal" href="#other-public-implementations" id="id12">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<h2><a class="toc-backref" href="#id2">Background</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>(Previously: <a class="reference external" href="../spinningup/rl_intro3.html">Introduction to RL, Part 3</a>)</p>
<p>The key idea underlying policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return, until you arrive at the optimal policy.</p>
<div class="section" id="quick-facts">
<h3><a class="toc-backref" href="#id3">Quick Facts</a><a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>VPG is an on-policy algorithm.</li>
<li>VPG can be used for environments with either discrete or continuous action spaces.</li>
<li>The Spinning Up implementation of VPG supports parallelization with MPI.</li>
</ul>
</div>
<div class="section" id="key-equations">
<h3><a class="toc-backref" href="#id4">Key Equations</a><a class="headerlink" href="#key-equations" title="Permalink to this headline">¶</a></h3>
<p>Let <img class="math" src="../_images/math/80088cfe6126980142c5447a9cb12f69ee7fa333.svg" alt="\pi_{\theta}"/> denote a policy with parameters <img class="math" src="../_images/math/1a20bd03ccceae216a40cb69d3fb7a3970f6f275.svg" alt="\theta"/>, and <img class="math" src="../_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg" alt="J(\pi_{\theta})"/> denote the expected finite-horizon undiscounted return of the policy. The gradient of <img class="math" src="../_images/math/bac2be784363161f5e76c206e3319aa035c066fc.svg" alt="J(\pi_{\theta})"/> is</p>
<div class="math">
<p><img src="../_images/math/291ee3741c3153bf4c21de62cee4823a4af59f58.svg" alt="\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{
    \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A^{\pi_{\theta}}(s_t,a_t)
    },"/></p>
</div><p>where <img class="math" src="../_images/math/018b60fd9e01bf256b27f2b67a2399df216eb7b2.svg" alt="\tau"/> is a trajectory and <img class="math" src="../_images/math/6f35b77f54a120bfa69fcacac9269bc4c6263f76.svg" alt="A^{\pi_{\theta}}"/> is the advantage function for the current policy.</p>
<p>The policy gradient algorithm works by updating policy parameters via stochastic gradient ascent on policy performance:</p>
<div class="math">
<p><img src="../_images/math/9053f48788d67cbf5725c480779428ee8ed21f98.svg" alt="\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta_k})"/></p>
</div><p>Policy gradient implementations typically compute advantage function estimates based on the infinite-horizon discounted return, despite otherwise use the finite-horizon undiscounted policy gradient formula.</p>
</div>
<div class="section" id="exploration-vs-exploitation">
<h3><a class="toc-backref" href="#id5">Exploration vs. Exploitation</a><a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>VPG trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima.</p>
</div>
<div class="section" id="pseudocode">
<h3><a class="toc-backref" href="#id6">Pseudocode</a><a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><img src="../_images/math/47a7bd5139a29bc2d2dc85cef12bba4b07b1e831.svg" alt="\begin{algorithm}[H]
    \caption{Vanilla Policy Gradient Algorithm}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Compute policy update, either using standard gradient ascent,
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha_k \hat{g}_k,
        \end{equation*}
        or via another gradient ascent algorithm like Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}"/></p>
</div></div>
</div>
<div class="section" id="documentation">
<h2><a class="toc-backref" href="#id7">Documentation</a><a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.vpg">
<code class="descclassname">spinup.</code><code class="descname">vpg</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=50</em>, <em>gamma=0.99</em>, <em>pi_lr=0.0003</em>, <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>, <em>lam=0.97</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/vpg/vpg.html#vpg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.vpg" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="16%" />
<col width="24%" />
<col width="60%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">logp</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of taking actions <code class="docutils literal"><span class="pre">a_ph</span></code></div>
<div class="line">in states <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>. (Critical: make sure</div>
<div class="line">to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to VPG.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs of interaction (equivalent to
number of policy updates) to perform.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>pi_lr</strong> (<em>float</em>) &#8211; Learning rate for policy optimizer.</li>
<li><strong>vf_lr</strong> (<em>float</em>) &#8211; Learning rate for value function optimizer.</li>
<li><strong>train_v_iters</strong> (<em>int</em>) &#8211; Number of gradient descent steps to take on
value function per epoch.</li>
<li><strong>lam</strong> (<em>float</em>) &#8211; Lambda for GAE-Lambda. (Always between 0 and 1,
close to 1.)</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="saved-model-contents">
<h3><a class="toc-backref" href="#id8">Saved Model Contents</a><a class="headerlink" href="#saved-model-contents" title="Permalink to this headline">¶</a></h3>
<p>The computation graph saved by the logger includes:</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>This saved model can be accessed either by</p>
<ul class="simple">
<li>running the trained policy with the <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> tool,</li>
<li>or loading the whole saved graph into a program with <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.</li>
</ul>
</div>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id9">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<div class="section" id="relevant-papers">
<h3><a class="toc-backref" href="#id10">Relevant Papers</a><a class="headerlink" href="#relevant-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>, Sutton et al. 2000</li>
<li><a class="reference external" href="http://joschu.net/docs/thesis.pdf">Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs</a>, Schulman 2016(a)</li>
<li><a class="reference external" href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>, Duan et al. 2016</li>
<li><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016(b)</li>
</ul>
</div>
<div class="section" id="why-these-papers">
<h3><a class="toc-backref" href="#id11">Why These Papers?</a><a class="headerlink" href="#why-these-papers" title="Permalink to this headline">¶</a></h3>
<p>Sutton 2000 is included because it is a timeless classic of reinforcement learning theory, and contains references to the earlier work which led to modern policy gradients. Schulman 2016(a) is included because Chapter 2 contains a lucid introduction to the theory of policy gradient algorithms, including pseudocode. Duan 2016 is a clear, recent benchmark paper that shows how vanilla policy gradient in the deep RL setting (eg with neural network policies and Adam as the optimizer) compares with other deep RL algorithms. Schulman 2016(b) is included because our implementation of VPG makes use of Generalized Advantage Estimation for computing the policy gradient.</p>
</div>
<div class="section" id="other-public-implementations">
<h3><a class="toc-backref" href="#id12">Other Public Implementations</a><a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/vpg.py">rllab</a></li>
<li><a class="reference external" href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/pg">rllib (Ray)</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="trpo.html" class="btn btn-neutral float-right" title="Trust Region Policy Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../spinningup/bench.html" class="btn btn-neutral" title="Benchmarks for Spinning Up Implementations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>